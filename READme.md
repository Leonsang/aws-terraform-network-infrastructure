# üõ°Ô∏è Proyecto de Detecci√≥n de Fraude Financiero con Terraform

[![AWS](https://img.shields.io/badge/AWS-Powered-orange)](https://aws.amazon.com/)
[![Terraform](https://img.shields.io/badge/Terraform-v1.2.0+-blueviolet)](https://www.terraform.io/)
[![Kaggle](https://img.shields.io/badge/Dataset-Kaggle-blue)](https://www.kaggle.com/c/ieee-fraud-detection)
[![Licencia](https://img.shields.io/badge/Licencia-MIT-green)](LICENSE)

Este proyecto implementa un sistema de ingenier√≠a de datos para detecci√≥n de fraude financiero en AWS, utilizando Terraform para la infraestructura como c√≥digo. El sistema est√° dise√±ado siguiendo una arquitectura Lakehouse con tres zonas de datos y utiliza Athena para consultas SQL sobre datos en S3.

> **Nota sobre la Arquitectura Actual**: Para la implementaci√≥n inicial y pruebas, estamos utilizando una versi√≥n optimizada para la capa gratuita de AWS. Sin embargo, el proyecto est√° dise√±ado para escalar a una arquitectura completa de producci√≥n cuando sea necesario.

## üìã Tabla de Contenidos

- [Arquitectura del Sistema](#-arquitectura-del-sistema)
- [Dataset](#-dataset)
- [Estructura del Proyecto](#-estructura-del-proyecto)
- [Requisitos Previos](#-requisitos-previos)
- [Gu√≠a de Instalaci√≥n](#-gu√≠a-de-instalaci√≥n)
- [Componentes Principales](#-componentes-principales)
- [Costos y L√≠mites](#-costos-y-l√≠mites)
- [Documentaci√≥n Detallada](#-documentaci√≥n-detallada)
- [Soluci√≥n de Problemas](#-soluci√≥n-de-problemas)
- [Contribuir](#-contribuir)
- [Licencia](#-licencia)
- [Contacto](#-contacto)

## üèóÔ∏è Arquitectura del Sistema

El sistema sigue una arquitectura Lakehouse en AWS con tres zonas principales:

### Raw Zone (Bronze) ü•â

- **Prop√≥sito**: Almacenamiento de datos sin procesar
- **Implementaci√≥n**: Bucket S3 con catalogaci√≥n mediante Glue Crawler
- **Formato**: Original (CSV) o Parquet con compresi√≥n
- **Caracter√≠sticas**: Inmutabilidad, retenci√≥n configurable, particionamiento por fecha

### Processed Zone (Silver) ü•à

- **Prop√≥sito**: Datos limpios y transformados
- **Implementaci√≥n**: Bucket S3 con modelo estrella implementado
- **Procesamiento**: ETL gestionado con AWS Glue
- **Caracter√≠sticas**: Validaci√≥n de calidad, normalizaci√≥n, enriquecimiento

### Analytics Zone (Gold) ü•á

- **Prop√≥sito**: Vistas materializadas para an√°lisis
- **Implementaci√≥n**:
  - **Athena**: Consultas SQL sobre datos en S3
  - **Vistas Materializadas**: Optimizadas para consultas frecuentes
  - **M√©tricas**: Dashboard de KPIs de fraude
- **Caracter√≠sticas**:
  - Optimizaci√≥n para consultas SQL
  - Particionamiento inteligente
  - Compresi√≥n de datos
  - Cache de consultas frecuentes

### Flujo de Datos

```
Ingesta ‚Üí Raw Zone ‚Üí Procesamiento ‚Üí Processed Zone ‚Üí An√°lisis ‚Üí Analytics Zone
   ‚Üë                     ‚Üë                                 ‚Üì
Tiempo Real         Batch (Glue)                      Dashboards
(Kinesis)                                            (QuickSight)
                     ‚Üì
                Athena (SQL)
```

## üí∞ Costos y L√≠mites

El proyecto est√° optimizado para la capa gratuita de AWS:

### Servicios Incluidos

- **S3**: 5GB de almacenamiento
- **Lambda**: 1M de invocaciones/mes
- **CloudWatch**: 5GB de logs/mes
- **Glue**: 10,000 DPU-horas/mes
- **Athena**: 1TB de datos escaneados/mes

### Optimizaciones

- Compresi√≥n de datos en S3 (Parquet con SNAPPY)
- Particionamiento eficiente por fecha
- Limpieza autom√°tica de logs
- Monitoreo de costos
- Cache de consultas frecuentes en Athena
- Vistas materializadas para reducir costos

## üìÅ Estructura del Proyecto

```
.
‚îú‚îÄ‚îÄ infrastructure/           # Infraestructura como c√≥digo
‚îÇ   ‚îú‚îÄ‚îÄ environments/        # Configuraciones por ambiente
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dev/            # Ambiente de desarrollo
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ main.tf     # Configuraci√≥n principal
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ variables.tf # Variables del ambiente
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ outputs.tf   # Outputs del ambiente
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ terraform.tfvars # Valores de variables
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ backend.hcl  # Configuraci√≥n del backend
‚îÇ   ‚îî‚îÄ‚îÄ modules/            # M√≥dulos de infraestructura
‚îÇ       ‚îú‚îÄ‚îÄ storage/        # M√≥dulo para S3 buckets
‚îÇ       ‚îú‚îÄ‚îÄ processing/     # M√≥dulo para Glue, Lambda
‚îÇ       ‚îú‚îÄ‚îÄ analytics/      # M√≥dulo para Athena
‚îÇ       ‚îú‚îÄ‚îÄ security/       # M√≥dulo para IAM y pol√≠ticas
‚îÇ       ‚îî‚îÄ‚îÄ monitoring/     # M√≥dulo para CloudWatch
‚îú‚îÄ‚îÄ src/                    # C√≥digo fuente del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ data_ingestion/    # M√≥dulo de ingesta de datos
‚îÇ   ‚îú‚îÄ‚îÄ data_processing/   # M√≥dulo de procesamiento
‚îÇ   ‚îú‚îÄ‚îÄ model_training/    # M√≥dulo de entrenamiento
‚îÇ   ‚îî‚îÄ‚îÄ config/           # Archivos de configuraci√≥n
‚îú‚îÄ‚îÄ scripts/               # Scripts de utilidad
‚îÇ   ‚îú‚îÄ‚îÄ glue/             # Scripts de ETL para Glue
‚îÇ   ‚îú‚îÄ‚îÄ lambda/           # Funciones Lambda
‚îÇ   ‚îî‚îÄ‚îÄ athena/           # Scripts SQL para Athena
‚îú‚îÄ‚îÄ tests/                # Pruebas unitarias e integraci√≥n
‚îú‚îÄ‚îÄ docs/                 # Documentaci√≥n del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ setup/           # Gu√≠as de configuraci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ technical/       # Documentaci√≥n t√©cnica
‚îÇ   ‚îú‚îÄ‚îÄ context/         # Contexto del negocio
‚îÇ   ‚îî‚îÄ‚îÄ diagrams/        # Diagramas del sistema
‚îú‚îÄ‚îÄ data/                # Datos de ejemplo y pruebas
‚îú‚îÄ‚îÄ requirements.txt     # Dependencias del proyecto
‚îú‚îÄ‚îÄ LICENSE             # Licencia del proyecto
‚îî‚îÄ‚îÄ README.md           # Documentaci√≥n principal
```

## üîß Requisitos Previos

- [Terraform](https://www.terraform.io/downloads.html) v1.2.0 o superior
- [AWS CLI](https://aws.amazon.com/cli/) configurado con credenciales adecuadas
- [Python](https://www.python.org/downloads/) 3.9 o superior (para scripts locales)
- [Cuenta de Kaggle](https://www.kaggle.com/) con API key para descarga autom√°tica
- [Git](https://git-scm.com/downloads) para clonar el repositorio
- [AWS Athena](https://aws.amazon.com/athena/) (incluido en la capa gratuita)

## üöÄ Gu√≠a de Instalaci√≥n

### 1. Clonar el Repositorio

```bash
git clone https://github.com/Leonsang/fraud-detection-terraform.git
cd fraud-detection-terraform
```

### 2. Configurar Credenciales de AWS

```bash
export AWS_ACCESS_KEY_ID="tu_access_key"
export AWS_SECRET_ACCESS_KEY="tu_secret_key"
export AWS_DEFAULT_REGION="us-east-1"
```

En Windows PowerShell:

```powershell
$env:AWS_ACCESS_KEY_ID="tu_access_key"
$env:AWS_SECRET_ACCESS_KEY="tu_secret_key"
$env:AWS_DEFAULT_REGION="us-east-1"
```

### 3. Personalizar Variables

Revisa y modifica los valores en `infrastructure/environments/dev/terraform.tfvars` seg√∫n tus necesidades:

```hcl
# Configuraci√≥n general
aws_region   = "us-east-1"
project_name = "fraud-detection"
environment  = "dev"  # Cambia a "test" o "prod" seg√∫n sea necesario

# Configuraci√≥n de Kaggle
kaggle_username = "tu_usuario_kaggle"  # Reemplazar con tu usuario de Kaggle
kaggle_key      = "tu_api_key_kaggle"  # Reemplazar con tu API key de Kaggle

# Configuraci√≥n de Athena
athena_database_bucket = "fraud-detection-dev-athena-db"
athena_output_bucket  = "fraud-detection-dev-athena-output"
processed_bucket     = "fraud-detection-dev-processed"
```

### 4. Configurar Credenciales de Kaggle

Para la descarga autom√°tica del dataset IEEE-CIS Fraud Detection:

1. Crea una cuenta en [Kaggle](https://www.kaggle.com/) si a√∫n no tienes una
2. Ve a tu perfil > Account > API > Create New API Token
3. Esto descargar√° un archivo `kaggle.json` con tus credenciales
4. Copia el archivo a `src/config/kaggle.json`
5. Actualiza los valores en `infrastructure/environments/dev/terraform.tfvars`

### 5. Crear la Capa Lambda para Kaggle

```bash
# Navegar al directorio de scripts
cd scripts/lambda

# Dar permisos de ejecuci√≥n al script (en Linux/Mac)
chmod +x create_kaggle_layer.sh

# Ejecutar el script
./create_kaggle_layer.sh
```

### 6. Inicializar y Aplicar Terraform

```bash
# Navegar al directorio del ambiente de desarrollo
cd infrastructure/environments/dev

# Inicializar Terraform
terraform init

# Crear un plan de ejecuci√≥n
terraform plan -out=tfplan

# Aplicar la configuraci√≥n
terraform apply tfplan
```

### 7. Ejecutar la Descarga de Datos Manualmente (opcional)

```bash
# Navegar al directorio de scripts
cd ../../../scripts/lambda

# Dar permisos de ejecuci√≥n al script (en Linux/Mac)
chmod +x trigger_kaggle_download.sh

# Ejecutar el script
./trigger_kaggle_download.sh fraud-detection-dev-kaggle-downloader
```

### 8. Monitorear el Pipeline

Puedes monitorear el estado del pipeline a trav√©s de:

- **AWS Console**:
  - CloudWatch Dashboard
  - Athena Query Editor
  - Glue Jobs
- **CLI**:

  ```bash
  # Ver estado del job de Glue
  aws glue get-job-runs --job-name fraud-detection-dev-etl-job
  
  # Ver logs de la funci√≥n Lambda
  aws logs get-log-events --log-group-name /aws/lambda/fraud-detection-dev-kaggle-downloader --log-stream-name $(aws logs describe-log-streams --log-group-name /aws/lambda/fraud-detection-dev-kaggle-downloader --query 'logStreams[0].logStreamName' --output text)
  
  # Ver consultas de Athena
  aws athena list-query-executions --work-group fraud-detection-dev-workgroup
  ```

### 9. Destruir la Infraestructura (cuando ya no se necesite)

```bash
terraform destroy
```

## üß© Componentes Principales

### M√≥dulos de Infraestructura üèóÔ∏è

#### Storage (`infrastructure/modules/storage/`) üíæ

- **Data Lake (S3)**
  - Zonas Raw, Processed y Analytics
  - Lifecycle policies por tipo de dato
  - Compresi√≥n y particionamiento inteligente
  - Integraci√≥n con AWS Lake Formation

#### Analytics (`infrastructure/modules/analytics/`) üìä

- **Athena**
  - Workgroup para ejecuci√≥n de consultas
  - Base de datos y tablas externas
  - Vistas materializadas
  - Pol√≠ticas de IAM
  - Monitoreo con CloudWatch

#### Processing (`infrastructure/modules/processing/`) ‚öôÔ∏è

- **Glue Jobs**
  - ETL de datos raw a processed
  - Transformaciones de datos
  - Particionamiento autom√°tico
  - Monitoreo de ejecuci√≥n

- **Lambda Functions**
  - Descarga de datos de Kaggle
  - Procesamiento en tiempo real
  - Triggers de ETL

## üîç Soluci√≥n de Problemas

### Problemas Comunes

1. **Error de Permisos en Athena**

   ```bash
   # Verificar pol√≠ticas IAM
   aws iam get-role --role-name fraud-detection-dev-athena-role
   
   # Verificar permisos S3
   aws s3 ls s3://fraud-detection-dev-processed/
   ```

2. **Consultas Lentas en Athena**
   - Verificar particionamiento de tablas
   - Optimizar formato de datos (Parquet)
   - Revisar estad√≠sticas de tablas

3. **Errores en Glue Jobs**

   ```bash
   # Ver logs detallados
   aws glue get-job-run --job-name fraud-detection-dev-etl-job --run-id <run-id>
   ```

## ü§ù Contribuir

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## üìÑ Licencia

Este proyecto est√° licenciado bajo la Licencia MIT - ver el archivo [LICENSE](LICENSE) para m√°s detalles.

## üìû Contacto

Tu Nombre - [@tutwitter](https://twitter.com/tutwitter) - <email@example.com>

Link del Proyecto: [https://github.com/Leonsang/fraud-detection-terraform](https://github.com/Leonsang/fraud-detection-terraform)

## üìä Notebooks y An√°lisis Pr√°cticos

### 1. An√°lisis Exploratorio üîç

#### Notebook: `notebooks/01_exploratory/fraud_patterns.ipynb`

```python
# An√°lisis de patrones de fraude
import pandas as pd
import plotly.express as px
from awswrangler import s3

# Cargar datos de diferentes fuentes
transactions = s3.read_parquet("s3://fraud-detection/processed/transactions/")
identity = s3.read_parquet("s3://fraud-detection/processed/identity/")
crypto = s3.read_parquet("s3://fraud-detection/processed/crypto/")

# An√°lisis de correlaciones
correlation_matrix = px.imshow(transactions.corr())
correlation_matrix.show()

# An√°lisis temporal de fraudes
fraud_timeline = px.line(transactions.groupby('date')['is_fraud'].mean())
fraud_timeline.show()
```

### 2. Feature Engineering üõ†Ô∏è

#### Notebook: `notebooks/02_features/feature_creation.ipynb`

```python
# Creaci√≥n de features avanzados
from fraud_detection.features import create_transaction_features
from fraud_detection.features import create_identity_features

# Features basados en ventanas temporales
transaction_features = create_transaction_features(
    transactions,
    windows=[1, 3, 7, 30],  # d√≠as
    aggregations=['mean', 'std', 'max']
)

# Features de redes
identity_features = create_identity_features(
    transactions,
    identity,
    network_params={
        'min_transactions': 3,
        'time_window_days': 30
    }
)
```

### 3. Modelado con SageMaker ü§ñ

#### Notebook: `notebooks/03_modeling/model_training.ipynb`

```python
# Entrenamiento del modelo
import sagemaker
from sagemaker.xgboost import XGBoost

# Configurar hiperpar√°metros
hyperparameters = {
    'max_depth': 6,
    'eta': 0.3,
    'gamma': 4,
    'min_child_weight': 6,
    'subsample': 0.8,
    'objective': 'binary:logistic',
    'num_round': 100
}

# Crear y entrenar el modelo
xgb_model = XGBoost(
    entry_point='train.py',
    framework_version='1.5-1',
    hyperparameters=hyperparameters,
    role=role,
    instance_count=1,
    instance_type='ml.m5.xlarge'
)

xgb_model.fit({
    'train': 's3://fraud-detection/train',
    'validation': 's3://fraud-detection/validation'
})
```

### 4. Evaluaci√≥n y Monitoreo üìà

#### Notebook: `notebooks/04_evaluation/model_evaluation.ipynb`

```python
# Evaluaci√≥n del modelo
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Matriz de confusi√≥n
cm = confusion_matrix(y_test, predictions)
sns.heatmap(cm, annot=True, fmt='d')
plt.show()

# Curva ROC
roc_curve = px.line(
    x=fpr, y=tpr,
    title='Curva ROC',
    labels={'x': 'Tasa de Falsos Positivos', 'y': 'Tasa de Verdaderos Positivos'}
)
roc_curve.show()

# Feature Importance
feature_importance = px.bar(
    x=feature_names,
    y=model.feature_importances_,
    title='Importancia de Features'
)
feature_importance.show()
```

### 5. An√°lisis de Datos No Estructurados üìù

#### Notebook: `notebooks/05_unstructured/document_analysis.ipynb`
