{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluación de Modelos en AWS - Detección de Fraude\n",
        "\n",
        "Este notebook implementa la evaluación detallada de modelos de detección de fraude usando servicios de AWS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuración del Entorno AWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import boto3\n",
        "import sagemaker\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
        "import json\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# Configurar logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Inicializar clientes AWS\n",
        "s3 = boto3.client('s3')\n",
        "cloudwatch = boto3.client('cloudwatch')\n",
        "sagemaker_runtime = boto3.client('sagemaker-runtime')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuración de Parámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar parámetros\n",
        "BUCKET = 'tu-bucket-s3'\n",
        "PREFIX = 'fraud-detection'\n",
        "MODEL_VERSION = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# Rutas de datos\n",
        "TEST_DATA_PATH = f's3://{BUCKET}/{PREFIX}/data/test/'\n",
        "RESULTS_PATH = f's3://{BUCKET}/{PREFIX}/evaluation/'\n",
        "\n",
        "# Configuración de métricas\n",
        "METRIC_NAMESPACE = 'FraudDetection/ModelEvaluation'\n",
        "THRESHOLD = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Carga de Datos y Predicciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_test_data():\n",
        "    try:\n",
        "        # Cargar datos de prueba\n",
        "        response = s3.get_object(Bucket=BUCKET, Key=f'{PREFIX}/data/test/test.csv')\n",
        "        test_data = pd.read_csv(response['Body'])\n",
        "        \n",
        "        X_test = test_data.drop('Class', axis=1)\n",
        "        y_test = test_data['Class']\n",
        "        \n",
        "        logger.info(f\"Datos de prueba cargados: {len(test_data)} registros\")\n",
        "        return X_test, y_test\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al cargar datos: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def get_predictions(endpoint_name, X_test):\n",
        "    try:\n",
        "        # Obtener predicciones del endpoint\n",
        "        predictions = []\n",
        "        for batch in np.array_split(X_test, 100):  # Procesar en lotes\n",
        "            response = sagemaker_runtime.invoke_endpoint(\n",
        "                EndpointName=endpoint_name,\n",
        "                Body=json.dumps(batch.tolist())\n",
        "            )\n",
        "            result = json.loads(response['Body'].read().decode())\n",
        "            predictions.extend(result)\n",
        "            \n",
        "        logger.info(f\"Predicciones obtenidas: {len(predictions)}\")\n",
        "        return np.array(predictions)\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al obtener predicciones: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "X_test, y_test = load_test_data()\n",
        "y_pred = get_predictions('fraud-detection-endpoint', X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Evaluación de Métricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_metrics(y_true, y_pred_proba):\n",
        "    try:\n",
        "        # Calcular métricas básicas\n",
        "        y_pred = (y_pred_proba >= THRESHOLD).astype(int)\n",
        "        \n        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y_true, y_pred),\n",
        "            'precision': precision_score(y_true, y_pred),\n",
        "            'recall': recall_score(y_true, y_pred),\n",
        "            'f1': f1_score(y_true, y_pred)\n",
        "        }\n",
        "        \n",
        "        # Calcular curvas ROC y PR\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
        "        precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
        "        \n",
        "        metrics['roc_auc'] = auc(fpr, tpr)\n",
        "        metrics['pr_auc'] = auc(recall, precision)\n",
        "        \n",
        "        # Publicar métricas en CloudWatch\n",
        "        cloudwatch.put_metric_data(\n",
        "            Namespace=METRIC_NAMESPACE,\n",
        "            MetricData=[\n",
        "                {\n",
        "                    'MetricName': name,\n",
        "                    'Value': value,\n",
        "                    'Unit': 'None',\n",
        "                    'Timestamp': datetime.now()\n",
        "                }\n",
        "                for name, value in metrics.items()\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "        logger.info(f\"Métricas calculadas: {metrics}\")\n",
        "        return metrics, (fpr, tpr), (precision, recall)\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al calcular métricas: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "metrics, roc_curve_data, pr_curve_data = calculate_metrics(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualización de Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_evaluation_curves(roc_data, pr_data):\n",
        "    try:\n",
        "        fpr, tpr = roc_data\n",
        "        precision, recall = pr_data\n",
        "        \n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "        \n",
        "        # Curva ROC\n",
        "        ax1.plot(fpr, tpr, label=f'ROC (AUC = {metrics[\"roc_auc\"]:.3f})')\n",
        "        ax1.plot([0, 1], [0, 1], 'k--')\n",
        "        ax1.set_xlabel('False Positive Rate')\n",
        "        ax1.set_ylabel('True Positive Rate')\n",
        "        ax1.set_title('Curva ROC')\n",
        "        ax1.legend()\n",
        "        \n",
        "        # Curva Precision-Recall\n",
        "        ax2.plot(recall, precision, label=f'PR (AUC = {metrics[\"pr_auc\"]:.3f})')\n",
        "        ax2.set_xlabel('Recall')\n",
        "        ax2.set_ylabel('Precision')\n",
        "        ax2.set_title('Curva Precision-Recall')\n",
        "        ax2.legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # Guardar gráficos\n",
        "        plt.savefig('/tmp/evaluation_curves.png')\n",
        "        s3.upload_file(\n",
        "            '/tmp/evaluation_curves.png',\n",
        "            BUCKET,\n",
        "            f'{PREFIX}/evaluation/plots/evaluation_curves_{MODEL_VERSION}.png'\n",
        "        )\n",
        "        \n",
        "        logger.info(\"Gráficos generados y guardados\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al generar gráficos: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "plot_evaluation_curves(roc_curve_data, pr_curve_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Análisis de Errores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_errors(X_test, y_test, y_pred):\n",
        "    try:\n",
        "        # Identificar errores\n",
        "        errors = X_test.copy()\n",
        "        errors['true_class'] = y_test\n",
        "        errors['predicted_class'] = (y_pred >= THRESHOLD).astype(int)\n",
        "        errors['error'] = errors['true_class'] != errors['predicted_class']\n",
        "        errors['confidence'] = y_pred\n",
        "        \n",
        "        # Analizar falsos positivos y negativos\n",
        "        fp = errors[(errors['true_class'] == 0) & (errors['predicted_class'] == 1)]\n",
        "        fn = errors[(errors['true_class'] == 1) & (errors['predicted_class'] == 0)]\n",
        "        \n",
        "        # Calcular estadísticas de errores\n",
        "        error_stats = {\n",
        "            'total_errors': len(errors[errors['error']]),\n",
        "            'false_positives': len(fp),\n",
        "            'false_negatives': len(fn),\n",
        "            'fp_confidence_mean': fp['confidence'].mean(),\n",
        "            'fn_confidence_mean': fn['confidence'].mean()\n",
        "        }\n",
        "        \n",
        "        # Guardar análisis de errores\n",
        "        s3.put_object(\n",
        "            Bucket=BUCKET,\n",
        "            Key=f'{PREFIX}/evaluation/error_analysis_{MODEL_VERSION}.json',\n",
        "            Body=json.dumps(error_stats, indent=2)\n",
        "        )\n",
        "        \n",
        "        logger.info(f\"Análisis de errores completado: {error_stats}\")\n",
        "        return error_stats\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en análisis de errores: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "error_analysis = analyze_errors(X_test, y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Generación de Reporte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_report(metrics, error_analysis):\n",
        "    try:\n",
        "        # Crear reporte completo\n",
        "        report = {\n",
        "            'evaluation_date': datetime.now().isoformat(),\n",
        "            'model_version': MODEL_VERSION,\n",
        "            'metrics': metrics,\n",
        "            'error_analysis': error_analysis,\n",
        "            'threshold': THRESHOLD,\n",
        "            'artifacts': {\n",
        "                'curves_plot': f'{PREFIX}/evaluation/plots/evaluation_curves_{MODEL_VERSION}.png',\n",
        "                'error_analysis': f'{PREFIX}/evaluation/error_analysis_{MODEL_VERSION}.json'\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Guardar reporte en S3\n",
        "        s3.put_object(\n",
        "            Bucket=BUCKET,\n",
        "            Key=f'{PREFIX}/evaluation/report_{MODEL_VERSION}.json',\n",
        "            Body=json.dumps(report, indent=2)\n",
        "        )\n",
        "        \n",
        "        logger.info(\"Reporte de evaluación generado exitosamente\")\n",
        "        return report\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al generar reporte: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "evaluation_report = generate_report(metrics, error_analysis)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
} 