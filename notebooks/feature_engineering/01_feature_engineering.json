{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ingeniería de Características para AWS - Detección de Fraude\n",
        "\n",
        "Este notebook implementa la ingeniería de características optimizada para AWS, utilizando PySpark y las mejores prácticas de procesamiento de datos distribuido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuración del Entorno AWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from awsglue.transforms import *\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from pyspark.context import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "from awsglue.dynamicframe import DynamicFrame\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import StandardScaler, PCA, VectorAssembler\n",
        "import boto3\n",
        "import logging\n",
        "\n",
        "# Configurar logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Inicializar contexto de Glue\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "job = Job(glueContext)\n",
        "job.init(args['JOB_NAME'], args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuración de Parámetros AWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar parámetros de AWS\n",
        "S3_BUCKET = 'tu-bucket-s3'\n",
        "INPUT_PATH = f's3://{S3_BUCKET}/raw/creditcard.csv'\n",
        "OUTPUT_PATH = f's3://{S3_BUCKET}/processed/'\n",
        "DATABASE_NAME = 'fraud_detection_db'\n",
        "TABLE_NAME = 'transactions'\n",
        "\n",
        "# Crear cliente S3\n",
        "s3_client = boto3.client('s3')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Carga y Validación de Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    try:\n",
        "        # Cargar datos usando Glue Dynamic Frame\n",
        "        dynamic_frame = glueContext.create_dynamic_frame.from_catalog(\n",
        "            database=DATABASE_NAME,\n",
        "            table_name=TABLE_NAME\n",
        "        )\n",
        "        \n",
        "        # Convertir a DataFrame de Spark\n",
        "        df = dynamic_frame.toDF()\n",
        "        \n",
        "        logger.info(f\"Datos cargados exitosamente. Total de registros: {df.count()}\")\n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al cargar datos: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Cargar datos\n",
        "df = load_data()\n",
        "\n",
        "# Validar esquema y calidad de datos\n",
        "def validate_data(df):\n",
        "    try:\n",
        "        # Verificar columnas requeridas\n",
        "        required_columns = ['Time', 'Amount'] + [f'V{i}' for i in range(1, 29)] + ['Class']\n",
        "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "        \n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"Columnas faltantes: {missing_columns}\")\n",
        "        \n",
        "        # Verificar tipos de datos\n",
        "        numeric_columns = ['Amount'] + [f'V{i}' for i in range(1, 29)]\n",
        "        for col in numeric_columns:\n",
        "            if not isinstance(df.schema[col].dataType, (DoubleType, FloatType, IntegerType)):\n",
        "                raise ValueError(f\"Columna {col} debe ser numérica\")\n",
        "        \n",
        "        # Verificar valores nulos\n",
        "        null_counts = {col: df.filter(df[col].isNull()).count() for col in df.columns}\n",
        "        if any(count > 0 for count in null_counts.values()):\n",
        "            logger.warning(f\"Valores nulos encontrados: {null_counts}\")\n",
        "            \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en validación de datos: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "validate_data(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Creación de Características"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_features(df):\n",
        "    try:\n",
        "        # Características temporales\n",
        "        df = df.withColumn('timestamp', F.from_unixtime(F.col('Time')))\n",
        "        df = df.withColumn('hour', F.hour('timestamp'))\n",
        "        df = df.withColumn('day', F.dayofmonth('timestamp'))\n",
        "        df = df.withColumn('dayofweek', F.dayofweek('timestamp'))\n",
        "        \n        # Características de monto\n",
        "        df = df.withColumn('amount_log', F.log1p('Amount'))\n",
        "        df = df.withColumn('amount_scaled', F.col('Amount') / F.max('Amount').over())\n",
        "        \n",
        "        # Características estadísticas\n",
        "        v_cols = [f'V{i}' for i in range(1, 29)]\n",
        "        df = df.withColumn('v_mean', sum([F.col(c) for c in v_cols]) / len(v_cols))\n",
        "        \n",
        "        # Características de interacción\n",
        "        df = df.withColumn('amount_hour_interaction', F.col('amount_scaled') * F.col('hour'))\n",
        "        \n",
        "        logger.info(\"Características creadas exitosamente\")\n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en creación de características: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "df_features = create_features(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Preparación para ML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_features(df):\n",
        "    try:\n",
        "        # Seleccionar características para ML\n",
        "        feature_cols = [\n",
        "            'amount_scaled', 'amount_log', 'hour', 'dayofweek',\n",
        "            'v_mean', 'amount_hour_interaction'\n",
        "        ] + [f'V{i}' for i in range(1, 29)]\n",
        "        \n",
        "        # Crear vector de características\n",
        "        assembler = VectorAssembler(\n",
        "            inputCols=feature_cols,\n",
        "            outputCol='features'\n",
        "        )\n",
        "        df_assembled = assembler.transform(df)\n",
        "        \n",
        "        # Escalar características\n",
        "        scaler = StandardScaler(\n",
        "            inputCol='features',\n",
        "            outputCol='features_scaled',\n",
        "            withStd=True,\n",
        "            withMean=True\n",
        "        )\n",
        "        df_scaled = scaler.fit(df_assembled).transform(df_assembled)\n",
        "        \n",
        "        # Aplicar PCA\n",
        "        pca = PCA(\n",
        "            k=10,\n",
        "            inputCol='features_scaled',\n",
        "            outputCol='features_pca'\n",
        "        )\n",
        "        df_pca = pca.fit(df_scaled).transform(df_scaled)\n",
        "        \n",
        "        logger.info(\"Características preparadas para ML\")\n",
        "        return df_pca, scaler, pca\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en preparación de características: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "df_prepared, scaler_model, pca_model = prepare_features(df_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Guardado de Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_results(df, scaler, pca):\n",
        "    try:\n",
        "        # Guardar DataFrame procesado\n",
        "        output_path = f\"{OUTPUT_PATH}/processed_features\"\n",
        "        df.write.mode('overwrite').parquet(output_path)\n",
        "        \n",
        "        # Guardar modelos de preprocesamiento\n",
        "        scaler.save(f\"{OUTPUT_PATH}/models/scaler\")\n",
        "        pca.save(f\"{OUTPUT_PATH}/models/pca\")\n",
        "        \n",
        "        # Guardar metadatos\n",
        "        metadata = {\n",
        "            'num_features': len(df.columns),\n",
        "            'num_samples': df.count(),\n",
        "            'feature_names': df.columns,\n",
        "            'processing_date': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        s3_client.put_object(\n",
        "            Bucket=S3_BUCKET,\n",
        "            Key='processed/metadata.json',\n",
        "            Body=json.dumps(metadata)\n",
        "        )\n",
        "        \n",
        "        logger.info(\"Resultados guardados exitosamente\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al guardar resultados: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "save_results(df_prepared, scaler_model, pca_model)\n",
        "\n",
        "# Finalizar job de Glue\n",
        "job.commit()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
} 